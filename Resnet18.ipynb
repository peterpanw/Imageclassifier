{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Resnet18.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOX5hO0U/HSIniqvW/tR277",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peterpanw/Imageclassifier/blob/main/Resnet18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QItaskvxB8J"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, layers, Sequential, optimizers, metrics   # Sequential 序贯模型\n",
        "import os    # os库  用于访问操作系统的标准库 处理文件和目录\n",
        "import numpy as np"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_apuK45xSzk",
        "outputId": "5075917e-d8fd-40e9-ba5c-f6c613dc5123"
      },
      "source": [
        "tf.random.set_seed(0)\n",
        "np.random.seed(0)\n",
        "os.environ['TF_CPP_MIN_LEVEL'] = '2'\n",
        "assert tf.__version__.startswith('2.')  # assert<表达式>  用于测试<表达式>的值，如果值为true正常通过，值为false则报错\"AssertError\"\n",
        "\n",
        "## 配置超参数\n",
        "batch_size = 128\n",
        "optimizer = optimizers.Adam(lr=0.0001)     #优化器\n",
        "epochs = 20"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xktHbSO5xWgM",
        "outputId": "7c03e20a-367c-4268-ff48-8c8a279f7683"
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "print(\"train_shape:\", x_train.shape, y_train.shape)\n",
        "y_train = tf.squeeze(y_train,  axis=1)\n",
        "y_test = tf.squeeze(y_test, axis=1)\n",
        "print(\"train_shape:\", x_train.shape, y_train.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 3s 0us/step\n",
            "170508288/170498071 [==============================] - 3s 0us/step\n",
            "train_shape: (50000, 32, 32, 3) (50000, 1)\n",
            "train_shape: (50000, 32, 32, 3) (50000,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "APYKr3YvxeGn"
      },
      "source": [
        "def preprocess(x,y):\n",
        "    x = tf.cast(x, dtype=tf.float32)/255.\n",
        "    y = tf.cast(y, dtype=tf.int32)\n",
        "    return x, y\n",
        "train_db = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_db = train_db.map(preprocess).shuffle(50000).batch(batch_size)\n",
        "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_db = test_db.map(preprocess).batch(batch_size)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJE5sX0exqWJ"
      },
      "source": [
        "####   构建 简单层\n",
        "class BasicBlock(layers.Layer):\n",
        "    def __init__(self, filter_num, strides=1):\n",
        "        super(BasicBlock, self).__init__()      # super()调用父类\n",
        "\n",
        "        \"\"\"\n",
        "        conv2d -> batchnormalization -> relu activation\n",
        "        \"\"\"\n",
        "        #unit1\n",
        "        self.conv1 = layers.Conv2D(filters=filter_num, kernel_size=(3, 3), strides=strides, padding='same')\n",
        "        self.bn1 = layers.BatchNormalization()\n",
        "        self.relu = layers.Activation('relu')\n",
        "\n",
        "        #unit2\n",
        "        self.conv2 = layers.Conv2D(filter_num, (3, 3), strides=1, padding='same')\n",
        "        self.bn2 = layers.BatchNormalization()\n",
        "\n",
        "        if strides != 1:\n",
        "            self.downsample = Sequential()\n",
        "            self.downsample.add(layers.Conv2D(filter_num, (1, 1), strides=strides))\n",
        "        else:\n",
        "            self.downsample = lambda x : x\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        x = inputs\n",
        "        # 前向传播\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        # 下采样\n",
        "        down = self.downsample(x)\n",
        "        # f(x)+x  对张量求和\n",
        "        out_put = layers.add([out, down])\n",
        "        out_put = tf.nn.relu(out_put)\n",
        "        return out_put"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcz0sMlCxwjG"
      },
      "source": [
        "class ResNet(keras.Model):\n",
        "    def __init__(self, layers_dims, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        #预处理层\n",
        "        self.stem = Sequential([layers.Conv2D(64, (3, 3), strides=(1, 1)),\n",
        "                                layers.BatchNormalization(),\n",
        "                                layers.Activation('relu'),\n",
        "                                layers.MaxPool2D((2, 2), strides=(1, 1), padding='same')])\n",
        "        #接上4个ResBlock层\n",
        "        self.resblock1 = self.ResBlock(64, blocks=layers_dims[0])\n",
        "        self.resblock2 = self.ResBlock(128, blocks=layers_dims[1], strides=2)\n",
        "        self.resblock3 = self.ResBlock(256, blocks=layers_dims[2], strides=2)\n",
        "        self.resblock4 = self.ResBlock(512, blocks=layers_dims[3], strides=2)\n",
        "\n",
        "        #分类层\n",
        "        self.avgpool = layers.GlobalAveragePooling2D()\n",
        "        self.fc = layers.Dense(num_classes)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        out = self.stem(inputs)\n",
        "        out = self.resblock1(out)\n",
        "        out = self.resblock2(out)\n",
        "        out = self.resblock3(out)\n",
        "        out = self.resblock4(out)\n",
        "        out = self.avgpool(out)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def ResBlock(self, filter_nums, blocks, strides=1):\n",
        "        resblock = Sequential()\n",
        "        resblock.add(BasicBlock(filter_nums, strides))\n",
        "        #  _ 在for循环中只是一个循环标志 类似于i, j\n",
        "        for _ in range(blocks):\n",
        "            resblock.add(BasicBlock(filter_nums, strides=1))\n",
        "        return resblock"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9cQRwHhx5GO",
        "outputId": "2fa01550-21ef-4340-bd30-b74448c8add3"
      },
      "source": [
        "# ResNet-18 18层卷积层   1+4*2*2+1   一个ResBlock包含两个BasicBlock，一个BasicBlock包含两个卷积层\n",
        "resnet_18 = ResNet([2, 2, 2, 2])\n",
        "\n",
        "\n",
        "# 测试网络输出shape\n",
        "# x = tf.random.normal((1, 32, 32, 3))\n",
        "# out = resnet_18(x)\n",
        "# print(out.shape)\n",
        "\n",
        "# 输出网络结构\n",
        "resnet_18.build(input_shape=(None, 32, 32, 3))\n",
        "# 输出参数 Param 计算过程\n",
        "resnet_18.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"res_net\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sequential (Sequential)     (None, 30, 30, 64)        2048      \n",
            "                                                                 \n",
            " sequential_1 (Sequential)   (None, 30, 30, 64)        223104    \n",
            "                                                                 \n",
            " sequential_2 (Sequential)   (None, 15, 15, 128)       823168    \n",
            "                                                                 \n",
            " sequential_4 (Sequential)   (None, 8, 8, 256)         3284736   \n",
            "                                                                 \n",
            " sequential_6 (Sequential)   (None, 4, 4, 512)         13123072  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  multiple                 0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17,461,258\n",
            "Trainable params: 17,449,610\n",
            "Non-trainable params: 11,648\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMwC4ZFox0ik",
        "outputId": "f188a0c9-6a18-435c-a5b5-fee0a64b64ce"
      },
      "source": [
        "def main():\n",
        "    for epoch in range(epochs):\n",
        "        for step, (x, y) in enumerate(train_db):       # 在训练集上训练 train_db\n",
        "            with tf.GradientTape() as tape:\n",
        "                logits = resnet_18(x)   # logits是网络输出层的输出\n",
        "                y_onehot = tf.one_hot(y, depth=10)   # 一维向量，标签\n",
        "                # tf.losses.categorical_crossentropy   先是正确值 再是预测值 否则loss优化会出错\n",
        "                loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n",
        "                loss = tf.reduce_mean(loss)\n",
        "            grads = tape.gradient(loss, resnet_18.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, resnet_18.trainable_variables))\n",
        "\n",
        "\n",
        "            if step % 10 == 0:\n",
        "                print(epoch, step, 'loss:', float(loss))\n",
        "\n",
        "            \"\"\"\n",
        "            ##  tf.argmax(input, axis)会根据axis取值的不同返回每行或者每列最大值的索引，\n",
        "            #  当axis=0时：比较每一列的元素，输出每一列最大元素所在的索引数组\n",
        "            #  当axis=1时：比较每一行的元素，输出每一行最大元素所在的索引数组\n",
        "            ## tf.equal(x, y) 用来判断两个矩阵或者向量相等的元素，相等返回true，反之返回false，返回的值得矩阵的维度=x=y的维度\n",
        "            ## tf.reduce_mean() 用来计算张量(tensor)沿着指定的数轴(tensor的某一维度)上的平均值，主要用于降维或者计算tensor的平均值\n",
        "            \"\"\"\n",
        "            if step % 50 == 0:\n",
        "                total_correct = 0\n",
        "                total_num = 0\n",
        "                for step, (x, y) in enumerate(test_db):         # 测试集 test_db\n",
        "                    logits = resnet_18(x)   # resnet网络的输出结果\n",
        "                    prob = tf.nn.softmax(logits, axis=1)    ## 经过softmax以后的 概率\n",
        "                    pred = tf.cast(tf.argmax(prob, axis=1), dtype=tf.int32)   ## 每一行的最大概率 ，\n",
        "                    correct = tf.reduce_sum(tf.cast(tf.equal(pred, y), dtype=tf.int32))\n",
        "\n",
        "                    total_correct += correct\n",
        "                    total_num += x.shape[0]\n",
        "                acc = total_correct/total_num    # 计算 准确率\n",
        "                print(epoch, step, 'acc:', float(acc))\n",
        "                resnet_18.save_weights('./checkpoint/weights.ckpt')\n",
        "                print('save weights')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0 loss: 2.2958498001098633\n",
            "0 78 acc: 0.1\n",
            "save weights\n",
            "0 10 loss: 2.3183469772338867\n",
            "0 20 loss: 2.2588086128234863\n",
            "0 30 loss: 2.297945022583008\n",
            "0 40 loss: 2.1125385761260986\n",
            "0 50 loss: 2.0396018028259277\n",
            "0 78 acc: 0.2212\n",
            "save weights\n",
            "0 60 loss: 2.0197649002075195\n",
            "0 70 loss: 1.9345688819885254\n",
            "0 80 loss: 1.9369703531265259\n",
            "0 90 loss: 1.7511627674102783\n",
            "0 100 loss: 1.9483706951141357\n",
            "0 78 acc: 0.3222\n",
            "save weights\n",
            "0 110 loss: 1.7691855430603027\n",
            "0 120 loss: 1.7771636247634888\n",
            "0 130 loss: 1.8035153150558472\n",
            "0 140 loss: 1.7614004611968994\n",
            "0 150 loss: 1.6852866411209106\n",
            "0 78 acc: 0.3466\n",
            "save weights\n",
            "0 160 loss: 1.670933723449707\n",
            "0 170 loss: 1.6317825317382812\n",
            "0 180 loss: 1.6690034866333008\n",
            "0 190 loss: 1.6325626373291016\n",
            "0 200 loss: 1.6812243461608887\n",
            "0 78 acc: 0.3951\n",
            "save weights\n",
            "0 210 loss: 1.7042170763015747\n",
            "0 220 loss: 1.7154488563537598\n",
            "0 230 loss: 1.622969150543213\n",
            "0 240 loss: 1.5559415817260742\n",
            "0 250 loss: 1.6120890378952026\n",
            "0 78 acc: 0.4169\n",
            "save weights\n",
            "0 260 loss: 1.4215830564498901\n",
            "0 270 loss: 1.531580924987793\n",
            "0 280 loss: 1.6455984115600586\n",
            "0 290 loss: 1.4759269952774048\n",
            "0 300 loss: 1.4481146335601807\n",
            "0 78 acc: 0.435\n",
            "save weights\n",
            "0 310 loss: 1.509252905845642\n",
            "0 320 loss: 1.4690905809402466\n",
            "0 330 loss: 1.541142463684082\n",
            "0 340 loss: 1.3409855365753174\n",
            "0 350 loss: 1.444931149482727\n",
            "0 78 acc: 0.4679\n",
            "save weights\n",
            "0 360 loss: 1.4496827125549316\n",
            "0 370 loss: 1.4455418586730957\n",
            "0 380 loss: 1.4793891906738281\n",
            "0 390 loss: 1.3886491060256958\n",
            "1 0 loss: 1.3589863777160645\n",
            "1 78 acc: 0.4903\n",
            "save weights\n",
            "1 10 loss: 1.3940629959106445\n",
            "1 20 loss: 1.4380278587341309\n",
            "1 30 loss: 1.3752089738845825\n",
            "1 40 loss: 1.2688533067703247\n",
            "1 50 loss: 1.3071801662445068\n",
            "1 78 acc: 0.473\n",
            "save weights\n",
            "1 60 loss: 1.3685269355773926\n",
            "1 70 loss: 1.3716380596160889\n",
            "1 80 loss: 1.3517348766326904\n",
            "1 90 loss: 1.2134472131729126\n",
            "1 100 loss: 1.4045181274414062\n",
            "1 78 acc: 0.52\n",
            "save weights\n",
            "1 110 loss: 1.1840903759002686\n",
            "1 120 loss: 1.3635523319244385\n",
            "1 130 loss: 1.5364254713058472\n",
            "1 140 loss: 1.3561935424804688\n",
            "1 150 loss: 1.160542368888855\n",
            "1 78 acc: 0.5306\n",
            "save weights\n",
            "1 160 loss: 1.1856876611709595\n",
            "1 170 loss: 1.1620404720306396\n",
            "1 180 loss: 1.2680511474609375\n",
            "1 190 loss: 1.1597706079483032\n",
            "1 200 loss: 1.2681688070297241\n",
            "1 78 acc: 0.5277\n",
            "save weights\n",
            "1 210 loss: 1.4035983085632324\n",
            "1 220 loss: 1.2779862880706787\n",
            "1 230 loss: 1.3158774375915527\n",
            "1 240 loss: 1.3329951763153076\n",
            "1 250 loss: 1.2995800971984863\n",
            "1 78 acc: 0.5602\n",
            "save weights\n",
            "1 260 loss: 1.1734249591827393\n",
            "1 270 loss: 1.3193695545196533\n",
            "1 280 loss: 1.2500102519989014\n",
            "1 290 loss: 1.1887712478637695\n",
            "1 300 loss: 1.348658561706543\n",
            "1 78 acc: 0.5687\n",
            "save weights\n",
            "1 310 loss: 1.2352831363677979\n",
            "1 320 loss: 1.2095415592193604\n",
            "1 330 loss: 1.3102233409881592\n",
            "1 340 loss: 1.1727721691131592\n",
            "1 350 loss: 1.2097935676574707\n",
            "1 78 acc: 0.5607\n",
            "save weights\n",
            "1 360 loss: 1.1090600490570068\n",
            "1 370 loss: 1.274902105331421\n",
            "1 380 loss: 1.2091107368469238\n",
            "1 390 loss: 1.3854646682739258\n",
            "2 0 loss: 1.2216897010803223\n",
            "2 78 acc: 0.5852\n",
            "save weights\n",
            "2 10 loss: 1.2173388004302979\n",
            "2 20 loss: 1.2849180698394775\n",
            "2 30 loss: 1.112276315689087\n",
            "2 40 loss: 1.1184918880462646\n",
            "2 50 loss: 1.105644702911377\n",
            "2 78 acc: 0.5865\n",
            "save weights\n",
            "2 60 loss: 1.243269681930542\n",
            "2 70 loss: 1.075769305229187\n",
            "2 80 loss: 1.2805380821228027\n",
            "2 90 loss: 1.1430556774139404\n",
            "2 100 loss: 1.0172405242919922\n",
            "2 78 acc: 0.5782\n",
            "save weights\n",
            "2 110 loss: 0.84675133228302\n",
            "2 120 loss: 1.0775047540664673\n",
            "2 130 loss: 1.1132625341415405\n",
            "2 140 loss: 1.0586931705474854\n",
            "2 150 loss: 0.997374415397644\n",
            "2 78 acc: 0.6075\n",
            "save weights\n",
            "2 160 loss: 1.0134785175323486\n",
            "2 170 loss: 0.9741528034210205\n",
            "2 180 loss: 1.0321906805038452\n",
            "2 190 loss: 0.9895201921463013\n",
            "2 200 loss: 0.9775340557098389\n",
            "2 78 acc: 0.5861\n",
            "save weights\n",
            "2 210 loss: 1.032383680343628\n",
            "2 220 loss: 1.0954670906066895\n",
            "2 230 loss: 1.316396713256836\n",
            "2 240 loss: 1.094888687133789\n",
            "2 250 loss: 0.9579631090164185\n",
            "2 78 acc: 0.5827\n",
            "save weights\n",
            "2 260 loss: 1.1269899606704712\n",
            "2 270 loss: 1.1416791677474976\n",
            "2 280 loss: 0.9676264524459839\n",
            "2 290 loss: 1.162138819694519\n",
            "2 300 loss: 1.027631163597107\n",
            "2 78 acc: 0.6214\n",
            "save weights\n",
            "2 310 loss: 1.137589931488037\n",
            "2 320 loss: 0.9103018641471863\n",
            "2 330 loss: 1.2975080013275146\n",
            "2 340 loss: 1.017233967781067\n",
            "2 350 loss: 0.9911651611328125\n",
            "2 78 acc: 0.6245\n",
            "save weights\n",
            "2 360 loss: 1.0270756483078003\n",
            "2 370 loss: 1.0513513088226318\n",
            "2 380 loss: 0.7580493688583374\n",
            "2 390 loss: 1.1042343378067017\n",
            "3 0 loss: 0.9142677187919617\n",
            "3 78 acc: 0.6247\n",
            "save weights\n",
            "3 10 loss: 1.0220019817352295\n",
            "3 20 loss: 0.7633358240127563\n",
            "3 30 loss: 0.9659618139266968\n",
            "3 40 loss: 0.8195372819900513\n",
            "3 50 loss: 0.9211609363555908\n",
            "3 78 acc: 0.6344\n",
            "save weights\n",
            "3 60 loss: 0.8681321740150452\n",
            "3 70 loss: 0.9795370101928711\n",
            "3 80 loss: 1.1768478155136108\n",
            "3 90 loss: 1.0522351264953613\n",
            "3 100 loss: 1.020922303199768\n",
            "3 78 acc: 0.6454\n",
            "save weights\n",
            "3 110 loss: 1.0643372535705566\n",
            "3 120 loss: 0.8465708494186401\n",
            "3 130 loss: 0.9619641900062561\n",
            "3 140 loss: 0.872388482093811\n",
            "3 150 loss: 0.7218000292778015\n",
            "3 78 acc: 0.6411\n",
            "save weights\n",
            "3 160 loss: 0.8272207379341125\n",
            "3 170 loss: 1.0996129512786865\n",
            "3 180 loss: 0.7870655655860901\n",
            "3 190 loss: 0.9431338906288147\n",
            "3 200 loss: 1.0408093929290771\n",
            "3 78 acc: 0.6078\n",
            "save weights\n",
            "3 210 loss: 1.0554447174072266\n",
            "3 220 loss: 0.9124714136123657\n",
            "3 230 loss: 0.9938551187515259\n",
            "3 240 loss: 0.939958930015564\n",
            "3 250 loss: 0.8621304035186768\n",
            "3 78 acc: 0.6516\n",
            "save weights\n",
            "3 260 loss: 0.8596049547195435\n",
            "3 270 loss: 0.8152062892913818\n",
            "3 280 loss: 0.9767663478851318\n",
            "3 290 loss: 0.8258434534072876\n",
            "3 300 loss: 0.9842028617858887\n",
            "3 78 acc: 0.6865\n",
            "save weights\n",
            "3 310 loss: 0.9797753095626831\n",
            "3 320 loss: 0.8851069211959839\n",
            "3 330 loss: 1.0507898330688477\n",
            "3 340 loss: 0.9293562173843384\n",
            "3 350 loss: 0.8715132474899292\n",
            "3 78 acc: 0.6548\n",
            "save weights\n",
            "3 360 loss: 0.823514997959137\n",
            "3 370 loss: 0.8341138362884521\n",
            "3 380 loss: 1.035832166671753\n",
            "3 390 loss: 1.0053207874298096\n",
            "4 0 loss: 0.9082009196281433\n",
            "4 78 acc: 0.644\n",
            "save weights\n",
            "4 10 loss: 0.7162660360336304\n",
            "4 20 loss: 0.9383330345153809\n",
            "4 30 loss: 0.776114284992218\n",
            "4 40 loss: 0.845343291759491\n",
            "4 50 loss: 0.8511626124382019\n",
            "4 78 acc: 0.6664\n",
            "save weights\n",
            "4 60 loss: 0.8485271334648132\n",
            "4 70 loss: 0.9000602960586548\n",
            "4 80 loss: 0.9318479299545288\n",
            "4 90 loss: 0.6320111155509949\n",
            "4 100 loss: 0.9081857800483704\n",
            "4 78 acc: 0.6794\n",
            "save weights\n",
            "4 110 loss: 0.6560629606246948\n",
            "4 120 loss: 0.760942816734314\n",
            "4 130 loss: 0.8250694274902344\n",
            "4 140 loss: 0.7092113494873047\n",
            "4 150 loss: 0.8985569477081299\n",
            "4 78 acc: 0.6509\n",
            "save weights\n",
            "4 160 loss: 0.8447810411453247\n",
            "4 170 loss: 0.6867319345474243\n",
            "4 180 loss: 0.6683107018470764\n",
            "4 190 loss: 0.8849175572395325\n",
            "4 200 loss: 0.7533168792724609\n",
            "4 78 acc: 0.6855\n",
            "save weights\n",
            "4 210 loss: 0.7973376512527466\n",
            "4 220 loss: 0.7830147743225098\n",
            "4 230 loss: 0.8062036633491516\n",
            "4 240 loss: 0.835176944732666\n",
            "4 250 loss: 0.6027398705482483\n",
            "4 78 acc: 0.6966\n",
            "save weights\n",
            "4 260 loss: 0.6449894905090332\n",
            "4 270 loss: 0.721656084060669\n",
            "4 280 loss: 0.6926521062850952\n",
            "4 290 loss: 0.6653394103050232\n",
            "4 300 loss: 0.8975139260292053\n",
            "4 78 acc: 0.7107\n",
            "save weights\n",
            "4 310 loss: 0.8336970806121826\n",
            "4 320 loss: 0.9729191064834595\n",
            "4 330 loss: 0.7161056995391846\n",
            "4 340 loss: 0.7192412614822388\n",
            "4 350 loss: 0.95923912525177\n",
            "4 78 acc: 0.7159\n",
            "save weights\n",
            "4 360 loss: 0.6647899746894836\n",
            "4 370 loss: 0.8369652032852173\n",
            "4 380 loss: 0.6837705373764038\n",
            "4 390 loss: 0.8115149736404419\n",
            "5 0 loss: 0.6636785268783569\n",
            "5 78 acc: 0.7036\n",
            "save weights\n",
            "5 10 loss: 0.588854968547821\n",
            "5 20 loss: 0.5876044034957886\n",
            "5 30 loss: 0.7030236124992371\n",
            "5 40 loss: 0.7975786328315735\n",
            "5 50 loss: 0.697455644607544\n",
            "5 78 acc: 0.7123\n",
            "save weights\n",
            "5 60 loss: 0.725977897644043\n",
            "5 70 loss: 0.6580904722213745\n",
            "5 80 loss: 0.5858163237571716\n",
            "5 90 loss: 0.6099853515625\n",
            "5 100 loss: 0.6192871332168579\n",
            "5 78 acc: 0.7065\n",
            "save weights\n",
            "5 110 loss: 0.6567355394363403\n",
            "5 120 loss: 0.64158034324646\n",
            "5 130 loss: 0.6359678506851196\n",
            "5 140 loss: 0.7288008332252502\n",
            "5 150 loss: 0.6756108999252319\n",
            "5 78 acc: 0.6917\n",
            "save weights\n",
            "5 160 loss: 0.9908102750778198\n",
            "5 170 loss: 0.6529224514961243\n",
            "5 180 loss: 0.716841995716095\n",
            "5 190 loss: 0.634974479675293\n",
            "5 200 loss: 0.7708526849746704\n",
            "5 78 acc: 0.7201\n",
            "save weights\n",
            "5 210 loss: 0.6075043678283691\n",
            "5 220 loss: 0.5989816188812256\n",
            "5 230 loss: 0.65343177318573\n",
            "5 240 loss: 0.5810656547546387\n",
            "5 250 loss: 0.7898476123809814\n",
            "5 78 acc: 0.7229\n",
            "save weights\n",
            "5 260 loss: 0.5231557488441467\n",
            "5 270 loss: 0.6424974203109741\n",
            "5 280 loss: 0.4913729429244995\n",
            "5 290 loss: 0.5907459855079651\n",
            "5 300 loss: 0.7320037484169006\n",
            "5 78 acc: 0.7306\n",
            "save weights\n",
            "5 310 loss: 0.695351243019104\n",
            "5 320 loss: 0.6531825065612793\n",
            "5 330 loss: 0.6554309129714966\n",
            "5 340 loss: 0.4855290353298187\n",
            "5 350 loss: 0.5411447882652283\n",
            "5 78 acc: 0.7468\n",
            "save weights\n",
            "5 360 loss: 0.5886656045913696\n",
            "5 370 loss: 0.7173224091529846\n",
            "5 380 loss: 0.678038477897644\n",
            "5 390 loss: 0.5340200662612915\n",
            "6 0 loss: 0.5447332859039307\n",
            "6 78 acc: 0.7165\n",
            "save weights\n",
            "6 10 loss: 0.5578082203865051\n",
            "6 20 loss: 0.590409517288208\n",
            "6 30 loss: 0.6310347318649292\n",
            "6 40 loss: 0.57170170545578\n",
            "6 50 loss: 0.491903692483902\n",
            "6 78 acc: 0.7378\n",
            "save weights\n",
            "6 60 loss: 0.46901175379753113\n",
            "6 70 loss: 0.6586533188819885\n",
            "6 80 loss: 0.7724616527557373\n",
            "6 90 loss: 0.5253713130950928\n",
            "6 100 loss: 0.6472304463386536\n",
            "6 78 acc: 0.7265\n",
            "save weights\n",
            "6 110 loss: 0.6493309140205383\n",
            "6 120 loss: 0.6903178691864014\n",
            "6 130 loss: 0.5298287868499756\n",
            "6 140 loss: 0.645363450050354\n",
            "6 150 loss: 0.573301374912262\n",
            "6 78 acc: 0.7142\n",
            "save weights\n",
            "6 160 loss: 0.5062394142150879\n",
            "6 170 loss: 0.3567718267440796\n",
            "6 180 loss: 0.5144978761672974\n",
            "6 190 loss: 0.49968045949935913\n",
            "6 200 loss: 0.583141028881073\n",
            "6 78 acc: 0.7201\n",
            "save weights\n",
            "6 210 loss: 0.5083425045013428\n",
            "6 220 loss: 0.548314094543457\n",
            "6 230 loss: 0.5447241067886353\n",
            "6 240 loss: 0.46364229917526245\n",
            "6 250 loss: 0.5687400102615356\n",
            "6 78 acc: 0.7526\n",
            "save weights\n",
            "6 260 loss: 0.4533708095550537\n",
            "6 270 loss: 0.5971055626869202\n",
            "6 280 loss: 0.39696580171585083\n",
            "6 290 loss: 0.5949407815933228\n",
            "6 300 loss: 0.3908246159553528\n",
            "6 78 acc: 0.7455\n",
            "save weights\n",
            "6 310 loss: 0.498138964176178\n",
            "6 320 loss: 0.4531092047691345\n",
            "6 330 loss: 0.5123339295387268\n",
            "6 340 loss: 0.7524604797363281\n",
            "6 350 loss: 0.6716073155403137\n",
            "6 78 acc: 0.7399\n",
            "save weights\n",
            "6 360 loss: 0.7360044717788696\n",
            "6 370 loss: 0.6634950637817383\n",
            "6 380 loss: 0.4707373082637787\n",
            "6 390 loss: 0.5218266248703003\n",
            "7 0 loss: 0.5032942891120911\n",
            "7 78 acc: 0.7322\n",
            "save weights\n",
            "7 10 loss: 0.5118662118911743\n",
            "7 20 loss: 0.4553638696670532\n",
            "7 30 loss: 0.38035517930984497\n",
            "7 40 loss: 0.43916749954223633\n",
            "7 50 loss: 0.40308132767677307\n",
            "7 78 acc: 0.7499\n",
            "save weights\n",
            "7 60 loss: 0.43947094678878784\n",
            "7 70 loss: 0.46969547867774963\n",
            "7 80 loss: 0.4565904140472412\n",
            "7 90 loss: 0.3730507791042328\n",
            "7 100 loss: 0.47890952229499817\n",
            "7 78 acc: 0.747\n",
            "save weights\n",
            "7 110 loss: 0.6195529103279114\n",
            "7 120 loss: 0.6703323125839233\n",
            "7 130 loss: 0.49233192205429077\n",
            "7 140 loss: 0.47082316875457764\n",
            "7 150 loss: 0.5528311729431152\n",
            "7 78 acc: 0.7491\n",
            "save weights\n",
            "7 160 loss: 0.6244498491287231\n",
            "7 170 loss: 0.5330022573471069\n",
            "7 180 loss: 0.44626516103744507\n",
            "7 190 loss: 0.5279346704483032\n",
            "7 200 loss: 0.49334973096847534\n",
            "7 78 acc: 0.7452\n",
            "save weights\n",
            "7 210 loss: 0.45675185322761536\n",
            "7 220 loss: 0.6622394323348999\n",
            "7 230 loss: 0.4752252697944641\n",
            "7 240 loss: 0.4675251245498657\n",
            "7 250 loss: 0.4414357542991638\n",
            "7 78 acc: 0.7464\n",
            "save weights\n",
            "7 260 loss: 0.481481671333313\n",
            "7 270 loss: 0.3829690217971802\n",
            "7 280 loss: 0.31333231925964355\n",
            "7 290 loss: 0.5455110669136047\n",
            "7 300 loss: 0.3545317053794861\n",
            "7 78 acc: 0.7531\n",
            "save weights\n",
            "7 310 loss: 0.2612946629524231\n",
            "7 320 loss: 0.4148256182670593\n",
            "7 330 loss: 0.44052207469940186\n",
            "7 340 loss: 0.33868497610092163\n",
            "7 350 loss: 0.5861181616783142\n",
            "7 78 acc: 0.7429\n",
            "save weights\n",
            "7 360 loss: 0.5503039360046387\n",
            "7 370 loss: 0.6145462989807129\n",
            "7 380 loss: 0.5224553346633911\n",
            "7 390 loss: 0.3497377336025238\n",
            "8 0 loss: 0.40297219157218933\n",
            "8 78 acc: 0.7517\n",
            "save weights\n",
            "8 10 loss: 0.3183440864086151\n",
            "8 20 loss: 0.3372471332550049\n",
            "8 30 loss: 0.32862409949302673\n",
            "8 40 loss: 0.5565986633300781\n",
            "8 50 loss: 0.35766738653182983\n",
            "8 78 acc: 0.7387\n",
            "save weights\n",
            "8 60 loss: 0.3791479170322418\n",
            "8 70 loss: 0.36553406715393066\n",
            "8 80 loss: 0.32216542959213257\n",
            "8 90 loss: 0.39717596769332886\n",
            "8 100 loss: 0.24237367510795593\n",
            "8 78 acc: 0.7587\n",
            "save weights\n",
            "8 110 loss: 0.30570054054260254\n",
            "8 120 loss: 0.3327869474887848\n",
            "8 130 loss: 0.3486614227294922\n",
            "8 140 loss: 0.3896483778953552\n",
            "8 150 loss: 0.4340536892414093\n",
            "8 78 acc: 0.7573\n",
            "save weights\n",
            "8 160 loss: 0.30368271470069885\n",
            "8 170 loss: 0.34693217277526855\n",
            "8 180 loss: 0.2670278549194336\n",
            "8 190 loss: 0.27231934666633606\n",
            "8 200 loss: 0.36734631657600403\n",
            "8 78 acc: 0.7565\n",
            "save weights\n",
            "8 210 loss: 0.3058614134788513\n",
            "8 220 loss: 0.3354066014289856\n",
            "8 230 loss: 0.3843485713005066\n",
            "8 240 loss: 0.397605299949646\n",
            "8 250 loss: 0.30452948808670044\n",
            "8 78 acc: 0.7614\n",
            "save weights\n",
            "8 260 loss: 0.36160606145858765\n",
            "8 270 loss: 0.3436814546585083\n",
            "8 280 loss: 0.39223891496658325\n",
            "8 290 loss: 0.36742401123046875\n",
            "8 300 loss: 0.4928564429283142\n",
            "8 78 acc: 0.7643\n",
            "save weights\n",
            "8 310 loss: 0.45853573083877563\n",
            "8 320 loss: 0.41848674416542053\n",
            "8 330 loss: 0.34072455763816833\n",
            "8 340 loss: 0.3665340542793274\n",
            "8 350 loss: 0.38991090655326843\n",
            "8 78 acc: 0.7573\n",
            "save weights\n",
            "8 360 loss: 0.31022918224334717\n",
            "8 370 loss: 0.5050618648529053\n",
            "8 380 loss: 0.35089290142059326\n",
            "8 390 loss: 0.3202560842037201\n",
            "9 0 loss: 0.2761388421058655\n",
            "9 78 acc: 0.7552\n",
            "save weights\n",
            "9 10 loss: 0.29432451725006104\n",
            "9 20 loss: 0.25930923223495483\n",
            "9 30 loss: 0.1110045462846756\n",
            "9 40 loss: 0.23993492126464844\n",
            "9 50 loss: 0.1439206898212433\n",
            "9 78 acc: 0.762\n",
            "save weights\n",
            "9 60 loss: 0.34867429733276367\n",
            "9 70 loss: 0.21177585422992706\n",
            "9 80 loss: 0.316732794046402\n",
            "9 90 loss: 0.17975856363773346\n",
            "9 100 loss: 0.3318597674369812\n",
            "9 78 acc: 0.7512\n",
            "save weights\n",
            "9 110 loss: 0.2501220107078552\n",
            "9 120 loss: 0.2874997556209564\n",
            "9 130 loss: 0.2155340313911438\n",
            "9 140 loss: 0.23840944468975067\n",
            "9 150 loss: 0.23799632489681244\n",
            "9 78 acc: 0.7509\n",
            "save weights\n",
            "9 160 loss: 0.40646815299987793\n",
            "9 170 loss: 0.18744301795959473\n",
            "9 180 loss: 0.2060147374868393\n",
            "9 190 loss: 0.19182172417640686\n",
            "9 200 loss: 0.24136437475681305\n",
            "9 78 acc: 0.7589\n",
            "save weights\n",
            "9 210 loss: 0.1776774525642395\n",
            "9 220 loss: 0.2890125811100006\n",
            "9 230 loss: 0.31557101011276245\n",
            "9 240 loss: 0.32957184314727783\n",
            "9 250 loss: 0.28241533041000366\n",
            "9 78 acc: 0.764\n",
            "save weights\n",
            "9 260 loss: 0.3395860493183136\n",
            "9 270 loss: 0.24017703533172607\n",
            "9 280 loss: 0.25177133083343506\n",
            "9 290 loss: 0.22417744994163513\n",
            "9 300 loss: 0.2703373432159424\n",
            "9 78 acc: 0.754\n",
            "save weights\n",
            "9 310 loss: 0.15182267129421234\n",
            "9 320 loss: 0.2862315773963928\n",
            "9 330 loss: 0.2940102517604828\n",
            "9 340 loss: 0.31634652614593506\n",
            "9 350 loss: 0.36146849393844604\n",
            "9 78 acc: 0.7504\n",
            "save weights\n",
            "9 360 loss: 0.22199857234954834\n",
            "9 370 loss: 0.22666752338409424\n",
            "9 380 loss: 0.3191477358341217\n",
            "9 390 loss: 0.24933628737926483\n",
            "10 0 loss: 0.26719486713409424\n",
            "10 78 acc: 0.7614\n",
            "save weights\n",
            "10 10 loss: 0.16644242405891418\n",
            "10 20 loss: 0.16648977994918823\n",
            "10 30 loss: 0.09920574724674225\n",
            "10 40 loss: 0.12754672765731812\n",
            "10 50 loss: 0.20693834125995636\n",
            "10 78 acc: 0.7661\n",
            "save weights\n",
            "10 60 loss: 0.33662348985671997\n",
            "10 70 loss: 0.1404344141483307\n",
            "10 80 loss: 0.2591485381126404\n",
            "10 90 loss: 0.14345131814479828\n",
            "10 100 loss: 0.1902342587709427\n",
            "10 78 acc: 0.7506\n",
            "save weights\n",
            "10 110 loss: 0.21106556057929993\n",
            "10 120 loss: 0.2241981476545334\n",
            "10 130 loss: 0.23908652365207672\n",
            "10 140 loss: 0.17297615110874176\n",
            "10 150 loss: 0.2542799711227417\n",
            "10 78 acc: 0.7625\n",
            "save weights\n",
            "10 160 loss: 0.3469480574131012\n",
            "10 170 loss: 0.15071821212768555\n",
            "10 180 loss: 0.19938308000564575\n",
            "10 190 loss: 0.30036765336990356\n",
            "10 200 loss: 0.2589015066623688\n",
            "10 78 acc: 0.7534\n",
            "save weights\n",
            "10 210 loss: 0.2138989418745041\n",
            "10 220 loss: 0.17445442080497742\n",
            "10 230 loss: 0.09897033125162125\n",
            "10 240 loss: 0.18365085124969482\n",
            "10 250 loss: 0.1764567643404007\n",
            "10 78 acc: 0.7617\n",
            "save weights\n",
            "10 260 loss: 0.17094336450099945\n",
            "10 270 loss: 0.21834136545658112\n",
            "10 280 loss: 0.2157975286245346\n",
            "10 290 loss: 0.1986117660999298\n",
            "10 300 loss: 0.20326562225818634\n",
            "10 78 acc: 0.7671\n",
            "save weights\n",
            "10 310 loss: 0.31871533393859863\n",
            "10 320 loss: 0.2711327075958252\n",
            "10 330 loss: 0.22856730222702026\n",
            "10 340 loss: 0.22418691217899323\n",
            "10 350 loss: 0.20635679364204407\n",
            "10 78 acc: 0.762\n",
            "save weights\n",
            "10 360 loss: 0.1301611363887787\n",
            "10 370 loss: 0.22433128952980042\n",
            "10 380 loss: 0.18843242526054382\n",
            "10 390 loss: 0.2622129023075104\n",
            "11 0 loss: 0.31803038716316223\n",
            "11 78 acc: 0.75\n",
            "save weights\n",
            "11 10 loss: 0.15031036734580994\n",
            "11 20 loss: 0.10311367362737656\n",
            "11 30 loss: 0.16671763360500336\n",
            "11 40 loss: 0.1127421110868454\n",
            "11 50 loss: 0.1402255892753601\n",
            "11 78 acc: 0.7619\n",
            "save weights\n",
            "11 60 loss: 0.09137433022260666\n",
            "11 70 loss: 0.12865114212036133\n",
            "11 80 loss: 0.14545738697052002\n",
            "11 90 loss: 0.06559588760137558\n",
            "11 100 loss: 0.22307126224040985\n",
            "11 78 acc: 0.7519\n",
            "save weights\n",
            "11 110 loss: 0.09900649636983871\n",
            "11 120 loss: 0.10142406821250916\n",
            "11 130 loss: 0.20721088349819183\n",
            "11 140 loss: 0.11503390222787857\n",
            "11 150 loss: 0.26066941022872925\n",
            "11 78 acc: 0.7618\n",
            "save weights\n",
            "11 160 loss: 0.23334798216819763\n",
            "11 170 loss: 0.13879737257957458\n",
            "11 180 loss: 0.13409340381622314\n",
            "11 190 loss: 0.2535431385040283\n",
            "11 200 loss: 0.20950917899608612\n",
            "11 78 acc: 0.7624\n",
            "save weights\n",
            "11 210 loss: 0.1430775225162506\n",
            "11 220 loss: 0.14675363898277283\n",
            "11 230 loss: 0.14515848457813263\n",
            "11 240 loss: 0.12728150188922882\n",
            "11 250 loss: 0.15276597440242767\n",
            "11 78 acc: 0.7616\n",
            "save weights\n",
            "11 260 loss: 0.14644113183021545\n",
            "11 270 loss: 0.17212432622909546\n",
            "11 280 loss: 0.17447781562805176\n",
            "11 290 loss: 0.22762589156627655\n",
            "11 300 loss: 0.18350011110305786\n",
            "11 78 acc: 0.7581\n",
            "save weights\n",
            "11 310 loss: 0.1588638871908188\n",
            "11 320 loss: 0.0705779641866684\n",
            "11 330 loss: 0.14824332296848297\n",
            "11 340 loss: 0.1535368114709854\n",
            "11 350 loss: 0.3911305069923401\n",
            "11 78 acc: 0.7618\n",
            "save weights\n",
            "11 360 loss: 0.16218222677707672\n",
            "11 370 loss: 0.16137729585170746\n",
            "11 380 loss: 0.08627073466777802\n",
            "11 390 loss: 0.2618672847747803\n",
            "12 0 loss: 0.09132667630910873\n",
            "12 78 acc: 0.7644\n",
            "save weights\n",
            "12 10 loss: 0.09358730912208557\n",
            "12 20 loss: 0.1580895632505417\n",
            "12 30 loss: 0.12369244545698166\n",
            "12 40 loss: 0.038187332451343536\n",
            "12 50 loss: 0.16618362069129944\n",
            "12 78 acc: 0.7665\n",
            "save weights\n",
            "12 60 loss: 0.0640466958284378\n",
            "12 70 loss: 0.08348393440246582\n",
            "12 80 loss: 0.09353978931903839\n",
            "12 90 loss: 0.11737196147441864\n",
            "12 100 loss: 0.10354913026094437\n",
            "12 78 acc: 0.7551\n",
            "save weights\n",
            "12 110 loss: 0.1220213919878006\n",
            "12 120 loss: 0.2063380479812622\n",
            "12 130 loss: 0.1129169911146164\n",
            "12 140 loss: 0.0941973403096199\n",
            "12 150 loss: 0.09752464294433594\n",
            "12 78 acc: 0.7564\n",
            "save weights\n",
            "12 160 loss: 0.1047925055027008\n",
            "12 170 loss: 0.08940188586711884\n",
            "12 180 loss: 0.10793399810791016\n",
            "12 190 loss: 0.10857391357421875\n",
            "12 200 loss: 0.0941120982170105\n",
            "12 78 acc: 0.7541\n",
            "save weights\n",
            "12 210 loss: 0.19260253012180328\n",
            "12 220 loss: 0.05493752658367157\n",
            "12 230 loss: 0.1488707810640335\n",
            "12 240 loss: 0.17020924389362335\n",
            "12 250 loss: 0.15112097561359406\n",
            "12 78 acc: 0.7601\n",
            "save weights\n",
            "12 260 loss: 0.21273073554039001\n",
            "12 270 loss: 0.08042847365140915\n",
            "12 280 loss: 0.07757473737001419\n",
            "12 290 loss: 0.11128696799278259\n",
            "12 300 loss: 0.20625996589660645\n",
            "12 78 acc: 0.7567\n",
            "save weights\n",
            "12 310 loss: 0.15233145654201508\n",
            "12 320 loss: 0.1183818131685257\n",
            "12 330 loss: 0.1279747486114502\n",
            "12 340 loss: 0.1165480986237526\n",
            "12 350 loss: 0.126522958278656\n",
            "12 78 acc: 0.7644\n",
            "save weights\n",
            "12 360 loss: 0.14468945562839508\n",
            "12 370 loss: 0.06485218554735184\n",
            "12 380 loss: 0.05745788291096687\n",
            "12 390 loss: 0.1264985352754593\n",
            "13 0 loss: 0.08989664912223816\n",
            "13 78 acc: 0.7559\n",
            "save weights\n",
            "13 10 loss: 0.07034973055124283\n",
            "13 20 loss: 0.13064147531986237\n",
            "13 30 loss: 0.06687474250793457\n",
            "13 40 loss: 0.05270621180534363\n",
            "13 50 loss: 0.03847641497850418\n",
            "13 78 acc: 0.7589\n",
            "save weights\n",
            "13 60 loss: 0.1406010091304779\n",
            "13 70 loss: 0.07912097126245499\n",
            "13 80 loss: 0.1445768177509308\n",
            "13 90 loss: 0.10335740447044373\n",
            "13 100 loss: 0.11930347234010696\n",
            "13 78 acc: 0.7613\n",
            "save weights\n",
            "13 110 loss: 0.09061554819345474\n",
            "13 120 loss: 0.14390066266059875\n",
            "13 130 loss: 0.05804131180047989\n",
            "13 140 loss: 0.1388077288866043\n",
            "13 150 loss: 0.01966516673564911\n",
            "13 78 acc: 0.7697\n",
            "save weights\n",
            "13 160 loss: 0.17597006261348724\n",
            "13 170 loss: 0.07048901915550232\n",
            "13 180 loss: 0.08124702423810959\n",
            "13 190 loss: 0.0796230286359787\n",
            "13 200 loss: 0.13614502549171448\n",
            "13 78 acc: 0.7675\n",
            "save weights\n",
            "13 210 loss: 0.09305614233016968\n",
            "13 220 loss: 0.060874421149492264\n",
            "13 230 loss: 0.07847738265991211\n",
            "13 240 loss: 0.1048826202750206\n",
            "13 250 loss: 0.07689057290554047\n",
            "13 78 acc: 0.7723\n",
            "save weights\n",
            "13 260 loss: 0.1438860297203064\n",
            "13 270 loss: 0.13994324207305908\n",
            "13 280 loss: 0.16545246541500092\n",
            "13 290 loss: 0.07433119416236877\n",
            "13 300 loss: 0.03143730014562607\n",
            "13 78 acc: 0.7638\n",
            "save weights\n",
            "13 310 loss: 0.10817114263772964\n",
            "13 320 loss: 0.06922577321529388\n",
            "13 330 loss: 0.12109610438346863\n",
            "13 340 loss: 0.07110501825809479\n",
            "13 350 loss: 0.1509317010641098\n",
            "13 78 acc: 0.7525\n",
            "save weights\n",
            "13 360 loss: 0.09506040811538696\n",
            "13 370 loss: 0.04739867523312569\n",
            "13 380 loss: 0.08576744794845581\n",
            "13 390 loss: 0.1692470759153366\n",
            "14 0 loss: 0.04011552035808563\n",
            "14 78 acc: 0.7685\n",
            "save weights\n",
            "14 10 loss: 0.05982078239321709\n",
            "14 20 loss: 0.0409674346446991\n",
            "14 30 loss: 0.03540922701358795\n",
            "14 40 loss: 0.009698629379272461\n",
            "14 50 loss: 0.056772004812955856\n",
            "14 78 acc: 0.7676\n",
            "save weights\n",
            "14 60 loss: 0.03559527546167374\n",
            "14 70 loss: 0.0287790410220623\n",
            "14 80 loss: 0.10789236426353455\n",
            "14 90 loss: 0.05273003131151199\n",
            "14 100 loss: 0.14852198958396912\n",
            "14 78 acc: 0.7642\n",
            "save weights\n",
            "14 110 loss: 0.0980980396270752\n",
            "14 120 loss: 0.03001115843653679\n",
            "14 130 loss: 0.06955692172050476\n",
            "14 140 loss: 0.12247887998819351\n",
            "14 150 loss: 0.08201010525226593\n",
            "14 78 acc: 0.7691\n",
            "save weights\n",
            "14 160 loss: 0.04875236377120018\n",
            "14 170 loss: 0.07077869027853012\n",
            "14 180 loss: 0.13839060068130493\n",
            "14 190 loss: 0.08784172683954239\n",
            "14 200 loss: 0.04027540236711502\n",
            "14 78 acc: 0.7509\n",
            "save weights\n",
            "14 210 loss: 0.11423291265964508\n",
            "14 220 loss: 0.07500308007001877\n",
            "14 230 loss: 0.06644967198371887\n",
            "14 240 loss: 0.11387866735458374\n",
            "14 250 loss: 0.07815476506948471\n",
            "14 78 acc: 0.7707\n",
            "save weights\n",
            "14 260 loss: 0.14402098953723907\n",
            "14 270 loss: 0.07850527763366699\n",
            "14 280 loss: 0.026789231225848198\n",
            "14 290 loss: 0.11714997887611389\n",
            "14 300 loss: 0.03231196478009224\n",
            "14 78 acc: 0.7754\n",
            "save weights\n",
            "14 310 loss: 0.034586042165756226\n",
            "14 320 loss: 0.05479208379983902\n",
            "14 330 loss: 0.12307842820882797\n",
            "14 340 loss: 0.1077810525894165\n",
            "14 350 loss: 0.03403070196509361\n",
            "14 78 acc: 0.7626\n",
            "save weights\n",
            "14 360 loss: 0.06463992595672607\n",
            "14 370 loss: 0.09877955913543701\n",
            "14 380 loss: 0.03976280614733696\n",
            "14 390 loss: 0.20323829352855682\n",
            "15 0 loss: 0.05136501044034958\n",
            "15 78 acc: 0.7719\n",
            "save weights\n",
            "15 10 loss: 0.05466591566801071\n",
            "15 20 loss: 0.048691052943468094\n",
            "15 30 loss: 0.015544632449746132\n",
            "15 40 loss: 0.03571879863739014\n",
            "15 50 loss: 0.031012840569019318\n",
            "15 78 acc: 0.7691\n",
            "save weights\n",
            "15 60 loss: 0.05727536976337433\n",
            "15 70 loss: 0.05532213672995567\n",
            "15 80 loss: 0.17445091903209686\n",
            "15 90 loss: 0.16852937638759613\n",
            "15 100 loss: 0.06306813657283783\n",
            "15 78 acc: 0.7421\n",
            "save weights\n",
            "15 110 loss: 0.03787422552704811\n",
            "15 120 loss: 0.058882929384708405\n",
            "15 130 loss: 0.13654747605323792\n",
            "15 140 loss: 0.042798273265361786\n",
            "15 150 loss: 0.12326932698488235\n",
            "15 78 acc: 0.7487\n",
            "save weights\n",
            "15 160 loss: 0.03759610280394554\n",
            "15 170 loss: 0.0401153638958931\n",
            "15 180 loss: 0.02457967773079872\n",
            "15 190 loss: 0.05258248746395111\n",
            "15 200 loss: 0.027202371507883072\n",
            "15 78 acc: 0.7733\n",
            "save weights\n",
            "15 210 loss: 0.04007508605718613\n",
            "15 220 loss: 0.028009941801428795\n",
            "15 230 loss: 0.09546054899692535\n",
            "15 240 loss: 0.04402724280953407\n",
            "15 250 loss: 0.08183970302343369\n",
            "15 78 acc: 0.771\n",
            "save weights\n",
            "15 260 loss: 0.07340022921562195\n",
            "15 270 loss: 0.06374569237232208\n",
            "15 280 loss: 0.03563070669770241\n",
            "15 290 loss: 0.1287257820367813\n",
            "15 300 loss: 0.07749348133802414\n",
            "15 78 acc: 0.7436\n",
            "save weights\n",
            "15 310 loss: 0.10801494121551514\n",
            "15 320 loss: 0.06367191672325134\n",
            "15 330 loss: 0.04956742376089096\n",
            "15 340 loss: 0.0451054647564888\n",
            "15 350 loss: 0.0185597762465477\n",
            "15 78 acc: 0.7687\n",
            "save weights\n",
            "15 360 loss: 0.012543156743049622\n",
            "15 370 loss: 0.025865523144602776\n",
            "15 380 loss: 0.08704306930303574\n",
            "15 390 loss: 0.1643090546131134\n",
            "16 0 loss: 0.07799755036830902\n",
            "16 78 acc: 0.7686\n",
            "save weights\n",
            "16 10 loss: 0.027012266218662262\n",
            "16 20 loss: 0.06951412558555603\n",
            "16 30 loss: 0.04894125461578369\n",
            "16 40 loss: 0.012459794990718365\n",
            "16 50 loss: 0.12663204967975616\n",
            "16 78 acc: 0.7625\n",
            "save weights\n",
            "16 60 loss: 0.03140693157911301\n",
            "16 70 loss: 0.041672274470329285\n",
            "16 80 loss: 0.0389321967959404\n",
            "16 90 loss: 0.030350785702466965\n",
            "16 100 loss: 0.028276965022087097\n",
            "16 78 acc: 0.7701\n",
            "save weights\n",
            "16 110 loss: 0.0569031797349453\n",
            "16 120 loss: 0.038849711418151855\n",
            "16 130 loss: 0.056527890264987946\n",
            "16 140 loss: 0.0664534792304039\n",
            "16 150 loss: 0.038770124316215515\n",
            "16 78 acc: 0.7571\n",
            "save weights\n",
            "16 160 loss: 0.027427520602941513\n",
            "16 170 loss: 0.02659374289214611\n",
            "16 180 loss: 0.1710677593946457\n",
            "16 190 loss: 0.057388920336961746\n",
            "16 200 loss: 0.05424131825566292\n",
            "16 78 acc: 0.7566\n",
            "save weights\n",
            "16 210 loss: 0.06078315153717995\n",
            "16 220 loss: 0.04915884509682655\n",
            "16 230 loss: 0.12305642664432526\n",
            "16 240 loss: 0.11207731813192368\n",
            "16 250 loss: 0.17853215336799622\n",
            "16 78 acc: 0.7547\n",
            "save weights\n",
            "16 260 loss: 0.021218618378043175\n",
            "16 270 loss: 0.0645037293434143\n",
            "16 280 loss: 0.1042913943529129\n",
            "16 290 loss: 0.08431804925203323\n",
            "16 300 loss: 0.017177071422338486\n",
            "16 78 acc: 0.7655\n",
            "save weights\n",
            "16 310 loss: 0.17567750811576843\n",
            "16 320 loss: 0.059004075825214386\n",
            "16 330 loss: 0.03253362327814102\n",
            "16 340 loss: 0.04659044370055199\n",
            "16 350 loss: 0.07843396067619324\n",
            "16 78 acc: 0.7636\n",
            "save weights\n",
            "16 360 loss: 0.18659205734729767\n",
            "16 370 loss: 0.12584389746189117\n",
            "16 380 loss: 0.10678786039352417\n",
            "16 390 loss: 0.05325635150074959\n",
            "17 0 loss: 0.051765356212854385\n",
            "17 78 acc: 0.7705\n",
            "save weights\n",
            "17 10 loss: 0.06104721128940582\n",
            "17 20 loss: 0.022744961082935333\n",
            "17 30 loss: 0.0904853418469429\n",
            "17 40 loss: 0.07301370054483414\n",
            "17 50 loss: 0.0662713423371315\n",
            "17 78 acc: 0.7643\n",
            "save weights\n",
            "17 60 loss: 0.11630883812904358\n",
            "17 70 loss: 0.008136947639286518\n",
            "17 80 loss: 0.04480942338705063\n",
            "17 90 loss: 0.022539617493748665\n",
            "17 100 loss: 0.011551431380212307\n",
            "17 78 acc: 0.7553\n",
            "save weights\n",
            "17 110 loss: 0.03471067547798157\n",
            "17 120 loss: 0.016149397939443588\n",
            "17 130 loss: 0.0561818927526474\n",
            "17 140 loss: 0.04931476712226868\n",
            "17 150 loss: 0.04072825610637665\n",
            "17 78 acc: 0.7654\n",
            "save weights\n",
            "17 160 loss: 0.05039919912815094\n",
            "17 170 loss: 0.021781137213110924\n",
            "17 180 loss: 0.04876798391342163\n",
            "17 190 loss: 0.027233179658651352\n",
            "17 200 loss: 0.06833052635192871\n",
            "17 78 acc: 0.7708\n",
            "save weights\n",
            "17 210 loss: 0.0903129130601883\n",
            "17 220 loss: 0.02177209034562111\n",
            "17 230 loss: 0.03716769441962242\n",
            "17 240 loss: 0.06399177759885788\n",
            "17 250 loss: 0.026140455156564713\n",
            "17 78 acc: 0.7537\n",
            "save weights\n",
            "17 260 loss: 0.09695921093225479\n",
            "17 270 loss: 0.14302562177181244\n",
            "17 280 loss: 0.11486265808343887\n",
            "17 290 loss: 0.05280287191271782\n",
            "17 300 loss: 0.045831579715013504\n",
            "17 78 acc: 0.7676\n",
            "save weights\n",
            "17 310 loss: 0.08225374668836594\n",
            "17 320 loss: 0.027869194746017456\n",
            "17 330 loss: 0.03129567950963974\n",
            "17 340 loss: 0.019399292767047882\n",
            "17 350 loss: 0.08116932958364487\n",
            "17 78 acc: 0.766\n",
            "save weights\n",
            "17 360 loss: 0.0845390111207962\n",
            "17 370 loss: 0.05664770305156708\n",
            "17 380 loss: 0.02543218433856964\n",
            "17 390 loss: 0.04564590007066727\n",
            "18 0 loss: 0.034833621233701706\n",
            "18 78 acc: 0.7531\n",
            "save weights\n",
            "18 10 loss: 0.04794491082429886\n",
            "18 20 loss: 0.10060857236385345\n",
            "18 30 loss: 0.005991306155920029\n",
            "18 40 loss: 0.0196361280977726\n",
            "18 50 loss: 0.09193665534257889\n",
            "18 78 acc: 0.7671\n",
            "save weights\n",
            "18 60 loss: 0.022084634751081467\n",
            "18 70 loss: 0.1423051655292511\n",
            "18 80 loss: 0.02651228755712509\n",
            "18 90 loss: 0.05624009668827057\n",
            "18 100 loss: 0.02595311775803566\n",
            "18 78 acc: 0.7582\n",
            "save weights\n",
            "18 110 loss: 0.13975612819194794\n",
            "18 120 loss: 0.15496593713760376\n",
            "18 130 loss: 0.02957528457045555\n",
            "18 140 loss: 0.03408794850111008\n",
            "18 150 loss: 0.02564990520477295\n",
            "18 78 acc: 0.7758\n",
            "save weights\n",
            "18 160 loss: 0.012163704261183739\n",
            "18 170 loss: 0.00841636210680008\n",
            "18 180 loss: 0.027524761855602264\n",
            "18 190 loss: 0.0551493838429451\n",
            "18 200 loss: 0.0322633758187294\n",
            "18 78 acc: 0.7649\n",
            "save weights\n",
            "18 210 loss: 0.058487024158239365\n",
            "18 220 loss: 0.12193849682807922\n",
            "18 230 loss: 0.06312358379364014\n",
            "18 240 loss: 0.05852670222520828\n",
            "18 250 loss: 0.07202039659023285\n",
            "18 78 acc: 0.7608\n",
            "save weights\n",
            "18 260 loss: 0.04042557254433632\n",
            "18 270 loss: 0.0604112409055233\n",
            "18 280 loss: 0.04816950857639313\n",
            "18 290 loss: 0.0807119756937027\n",
            "18 300 loss: 0.097739078104496\n",
            "18 78 acc: 0.7609\n",
            "save weights\n",
            "18 310 loss: 0.04994357377290726\n",
            "18 320 loss: 0.013473635539412498\n",
            "18 330 loss: 0.025223543867468834\n",
            "18 340 loss: 0.1199047863483429\n",
            "18 350 loss: 0.0746690183877945\n",
            "18 78 acc: 0.7776\n",
            "save weights\n",
            "18 360 loss: 0.07084599882364273\n",
            "18 370 loss: 0.013076492585241795\n",
            "18 380 loss: 0.03312159702181816\n",
            "18 390 loss: 0.02092384174466133\n",
            "19 0 loss: 0.022109780460596085\n",
            "19 78 acc: 0.776\n",
            "save weights\n",
            "19 10 loss: 0.0939629003405571\n",
            "19 20 loss: 0.029454723000526428\n",
            "19 30 loss: 0.04335080459713936\n",
            "19 40 loss: 0.004859165288507938\n",
            "19 50 loss: 0.08863190561532974\n",
            "19 78 acc: 0.7717\n",
            "save weights\n",
            "19 60 loss: 0.05415970832109451\n",
            "19 70 loss: 0.07457578927278519\n",
            "19 80 loss: 0.041558556258678436\n",
            "19 90 loss: 0.041607003659009933\n",
            "19 100 loss: 0.03812430799007416\n",
            "19 78 acc: 0.7621\n",
            "save weights\n",
            "19 110 loss: 0.012477325275540352\n",
            "19 120 loss: 0.03378112614154816\n",
            "19 130 loss: 0.021120375022292137\n",
            "19 140 loss: 0.009374196641147137\n",
            "19 150 loss: 0.059330932796001434\n",
            "19 78 acc: 0.7757\n",
            "save weights\n",
            "19 160 loss: 0.021590963006019592\n",
            "19 170 loss: 0.037816863507032394\n",
            "19 180 loss: 0.02481004036962986\n",
            "19 190 loss: 0.05525679886341095\n",
            "19 200 loss: 0.07604742050170898\n",
            "19 78 acc: 0.7618\n",
            "save weights\n",
            "19 210 loss: 0.06663098186254501\n",
            "19 220 loss: 0.05889967828989029\n",
            "19 230 loss: 0.05132060870528221\n",
            "19 240 loss: 0.07515885680913925\n",
            "19 250 loss: 0.060522183775901794\n",
            "19 78 acc: 0.7571\n",
            "save weights\n",
            "19 260 loss: 0.03845161199569702\n",
            "19 270 loss: 0.02639129012823105\n",
            "19 280 loss: 0.11078359186649323\n",
            "19 290 loss: 0.03393200784921646\n",
            "19 300 loss: 0.045213330537080765\n",
            "19 78 acc: 0.7675\n",
            "save weights\n",
            "19 310 loss: 0.09982696175575256\n",
            "19 320 loss: 0.02414722368121147\n",
            "19 330 loss: 0.057735104113817215\n",
            "19 340 loss: 0.0261576808989048\n",
            "19 350 loss: 0.06141485646367073\n",
            "19 78 acc: 0.7731\n",
            "save weights\n",
            "19 360 loss: 0.09596491605043411\n",
            "19 370 loss: 0.06149066239595413\n",
            "19 380 loss: 0.01760455034673214\n",
            "19 390 loss: 0.02013082429766655\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Vv3ooIzyIMQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}